{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = datasets.load_iris()\n",
    "x_data = data.data\n",
    "y_data = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.hstack((x_data, np.reshape(y_data,(150,-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data = pd.DataFrame(data[:,:-1], columns=['feature1', 'feature2','feature3','feature4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_data = pd.DataFrame(data[:,-1], columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train = x_data.loc[:124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train = y_data.loc[:124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_test = x_data.loc[125:]\n",
    "y_test = y_data.loc[125:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "def toLabel(dff, old_feature_name):\n",
    "    second = dff[old_feature_name].mean()\n",
    "    minimum = dff[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = dff[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return dff[old_feature_name].apply(label, args= (first, second, third))\n",
    "\n",
    "for cols in x_data.columns:\n",
    "    x_data[cols] = toLabel(x_data, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def predict(tree, x_test):\n",
    "    return traverse(tree, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_accuracy(tree,x_test, y_test):\n",
    "    sum = 0\n",
    "    for ix in range(x_test.shape[0]):\n",
    "        sum += (predict(tree,x_test.iloc[ix]) == y_test.iloc[ix])\n",
    "    return sum/x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_dr.csv')#importing the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:,:-1]#slicing tthe dataframe for the target and the tarining data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the class node for keeping the different values for the each node\n",
    "class node:\n",
    "    def __init__(self,n,ent,pent,l,df,y):\n",
    "        self.name=n\n",
    "        self.bf=None\n",
    "        self.entropy = ent\n",
    "        self.parentEntropy = pent\n",
    "        self.level=l\n",
    "        self.dataframe = df\n",
    "        self.target = y\n",
    "        self.children = []\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def getentropy(self):\n",
    "        return self.entropy\n",
    "    def getlevel(self):\n",
    "        return self.level\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to calculate the value of a given type in a partiular feature \n",
    "def countTrue(feature_name,val):\n",
    "    x = df[feature_name] == val\n",
    "    count = 0\n",
    "    for i in x:\n",
    "        if i == True:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing the pieces of the code here \n",
    "df=train\n",
    "y=target\n",
    "s=list(y)\n",
    "pc = pd.value_counts(y)\n",
    "pc = np.array(pc)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it would calculate the entropy based on the ouput coloumn y--by the count of the distinct values in the y for the value \n",
    "def entropyCalc(y):\n",
    "    pc =pd.value_counts(y)\n",
    "    pc = np.array(pc)\n",
    "    total = pc.sum()\n",
    "\n",
    "    entropy = 0\n",
    "    for i in pc:\n",
    "       \n",
    "        if(i>0):\n",
    "            entropy = entropy + (-1*(i/total)*math.log((i/total),2))\n",
    "        \n",
    "    return entropy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedEntropy(df,feature,y):\n",
    "    \n",
    "\n",
    "    values = set(df[feature])\n",
    "    # calculating the fraction of each type in a given feature \n",
    "    weight = {}\n",
    "    for i in values:\n",
    "        weight[i] = countTrue(feature,i)/df[feature].shape[0]\n",
    "   \n",
    "    # putting entropy corresponding to distinct values in a certain feature in dictionary called entropy\n",
    "    \n",
    "    entropy = {}    \n",
    "    for i in values:\n",
    "        entropy[i] = entropyCalc(y[df[feature]==i])\n",
    "  \n",
    "    \n",
    "    #calculating weighted entropy... \n",
    "        \n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    for i in values:\n",
    "        weighted_entropy = weighted_entropy + (weight[i]*entropy[i])\n",
    "    \n",
    "    return weighted_entropy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(parent_entropy, child_entropy):\n",
    "    return parent_entropy - child_entropy\n",
    "# information gains includes the difference in the parent entropy to the child node entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the count of distinct values in v\n",
    "def y_val(y):\n",
    "    s=list(y)\n",
    "    ans=set()\n",
    "    for i in s:\n",
    "        ans.add(i)\n",
    "    return len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countY(lst, x):\n",
    "    count = 0\n",
    "    for ele in lst:\n",
    "        if (ele == x):\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(df,y, unused_features,cur_node):\n",
    "    #base case\n",
    "    # 1. unused is empty\n",
    "    # 2. y contains only one distinct value\n",
    "    if(len(unused_features)==0 or y_val(y)==1):\n",
    "         return\n",
    "\n",
    "        \n",
    "    best_feature = \"\"\n",
    "    #print(unused_features)\n",
    "    \n",
    "    max_info_gain = 0\n",
    "    \n",
    "    for f in unused_features:\n",
    "        \n",
    "        weighted_entropy = weightedEntropy(df,f,y)\n",
    "        \n",
    "        info_gain = informationGain(cur_node.parentEntropy, weighted_entropy)\n",
    "        if(info_gain > max_info_gain):\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = f\n",
    "            \n",
    "   # here you should know the best feature\n",
    "   # print it out\n",
    "   # print(\"Best Feature \", best_feature)  \n",
    "    possible_values = set(df[best_feature])\n",
    "    cur_node.bf= best_feature\n",
    "    for val in possible_values:\n",
    "        \n",
    "        new_y=y[df[best_feature]==val]\n",
    "        new_df=df[df[best_feature]==val]\n",
    "        new_ent=entropyCalc(new_y)\n",
    "            \n",
    "        new_node=node(val,entropyCalc(new_y),cur_node.getentropy(),cur_node.getlevel()+1,new_df,new_y)\n",
    "            \n",
    "        cur_node.add_child(new_node)\n",
    "        \n",
    "    # remove best feature from unused features    \n",
    "    unused_features.remove(best_feature)\n",
    "   # print(unused_features)\n",
    "        \n",
    "    # loop over possible values of best feature    \n",
    "    for child in cur_node.children:\n",
    "        new_df = child.dataframe\n",
    "        new_y=child.target\n",
    "        \n",
    "        # call build tree recursively\n",
    "        build_tree(new_df,new_y,unused_features,child)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printtree(cur_node):\n",
    "    for c in cur_node.children:\n",
    "        print(\"Level \",c.level)\n",
    "        \n",
    "        ss=list(c.target)\n",
    "        f=False\n",
    "        if(c.bf !=None):\n",
    "                f=True\n",
    "            \n",
    "        \n",
    "        possible_outputs = set(c.target[ss])\n",
    "        \n",
    "        lst=list(c.target[ss])\n",
    "        for val in possible_outputs:\n",
    "            print(\"Count of \",val,\" = \",countY(lst,val))\n",
    "        print(\"Current Entropy  is = \",c.getentropy())\n",
    "        \n",
    "        if(f==True):\n",
    "            weighted_entropy = weightedEntropy(c.dataframe,c.bf,c.target)\n",
    "            info_gain = informationGain(c.entropy, weighted_entropy)\n",
    "            print(\"Splitting on feature \",cur_node.bf,\" with gain ratio \",info_gain)\n",
    "            print()\n",
    "        elif(f==False):\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "        printtree(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train\n",
    "y=target\n",
    "\n",
    "unused_features = set(df.columns)\n",
    "#making of the root node here \n",
    "cur_node=node('root',entropyCalc(y),entropyCalc(y),0,df,y)\n",
    "build_tree(df, y ,unused_features,cur_node)\n",
    "\n",
    "s=list(cur_node.target)\n",
    "# printing the root node here\n",
    "print(\"Level \",cur_node.getlevel())\n",
    "possible_outputs = set(s)\n",
    "print(possible_outputs)\n",
    "lst=list(s)\n",
    "for val in possible_outputs:\n",
    "    print(\"Count of \",val,\" = \",countY(lst,val))\n",
    "print(\"Current Entropy  is = \",cur_node.getentropy())\n",
    "weighted_entropy = weightedEntropy(cur_node.dataframe,cur_node.bf,cur_node.target)\n",
    "info_gain = informationGain(cur_node.entropy, weighted_entropy)\n",
    "print(\"Splitting on feature \",cur_node.bf,\" with gain ratio \",info_gain)\n",
    "print()\n",
    "# calling the function to call the print function for the rest of the nodes out there \n",
    "printtree(cur_node)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
